\documentclass[11pt]{article}
\usepackage{pgfplots}
\usepackage{url}
\pgfplotsset{compat=newest} 
\setlength\topmargin{-0.6cm}   
\setlength\textheight{23.4cm}
\setlength\textwidth{17.0cm}
\setlength\oddsidemargin{0cm} 
\begin{document}
\title{Ling 572 HW9}
\author{Daniel Campos  \tt {dacampos@uw.edu}}
\date{03/13/2019}
\maketitle 
\section{Q1}
\subsection{What does f'(x) intend to measure?}
The derrivative of a function measures the rate of change of a function rleative to the change in the agrument(in this case X).
\subsection{Let $h(x)=f(g(x))$ }
$h'(x)$ = $f'(g(x)) \cdot g'(x)$
\subsection{Let $h(x)=f(x)g(x)$ }
$h'(x) = f'(x)g(x) + f(x)g'(x)$
\subsection{Let $f(x)=a^x$ where $a>0$}
$ f'(x)$ =$ a^x log(a)$
\subsection{Let $f(x)= x^{10}-2x^8  \frac{4}{x^2} + 10$}
$f'(x) =10x^{9}-16x^7 + \frac{8}{x^3}$
\section{Q2}
The logistic function is $f(x)=\frac{1}{1+e^{-x}}$. The tanh function is $g(x)=\frac{e^x - e^{-x}}{e^x +e^{-x}}$.
\subsection{Prove that $f'(x)= f(x)(1-f(x))$}
1. $f'(x) = \frac{e^{-x}}{(1+e^{-x})^2}$ \\
2. $f'(x) =  \frac{1}{1+e^{-x}}-\frac{1}{(1+e^{-x})^2}$ \\
3. $f'(x) = \frac{1}{1+e^{-x}} \cdot (1-\frac{1}{1+e^{-x}})$  which means  $f'(x)= f(x)(1-f(x))$\\
\subsection{Prove that $g'(x)=1 - g^2(x)$}
1. $tanh(x) = \frac {sinh(x))}{cosh(x)}$ \\
2. $g'(x) = \frac{df}{dx} \frac{sinh(x)}{cosh(x)}$\\
3. $g'(x) = \frac{cosh^2(x) - sinh^2(x)}{cosh^2(x)}$\\
4. $g'(x) = \frac{cosh^2(x)}{cosh^2(x)} - {sinh^2(x)}{cosh^2(x)}$\\
5. $g'(x) = 1- {sinh^2(x)}{cosh^2(x)} $\\
6. $g'(x) = 1-tanh^2(x)$\\
\subsection{Prove that $g(x) = 2f(2x)-1$}
1. $f(2x) = \frac{1}{1+e^{-2x}}$ \\
2. $2f(2x) =\frac{2}{1+e^{-2x}}$ \\
3. $2f(2x) - 1 = \frac{2}{1+e^{-2x}}  -1$ \\
3. $2f(2x) - 1 = \frac{2}{1+e^{-2x}}  - \frac{1+e^{-2x}}{1+e^{-2x}}$ \\
4. $2f(2x) - 1 = \frac{2- e^{-2x}}{1+e^{-2x}}$ \\
5. $2f(2x) - 1 = \frac{(e^{x}-1)(e^{x}+1)}{1+e^{2x}}$ \\
6. $2f(2x) - 1 = \frac{e^{2x}-1)}{1+e^{2x}}$ \\
7.  $2f(2x) - 1  = \frac{e^{2x}-1}{e^{2x}+1} = tanh(x) = g(x)$  \\
\section{Q3}   
\subsection{What is $f'_x$ trying to measure?}
A partial derrivative is trying to measure the change in a fucntion based on a variable assuming all other varriables in the function remain constant. In other words, our derrivative is representing the effect of a variable on the equation when no other variables are effecting the equation.
\subsection{$f(x,y)=x^3 + 3x^2y+y^3 + 2x$.}
$f'_x = 3x^2 + 6xy+2 $  \\ $f'_y = 3(x^2+y^2)$
\subsection{ $z = \sum_{i=1}^n w_i x_i$.}
$\frac{dz}{dw_i}=$ \\
\subsection{$f(z)=\frac{1}{1+e^{-z}}$ and $z = \sum_{i=1}^n w_i x_i$.}
$\frac{df}{dz}= \frac{e^{-1}}{(1+e^{-1})^2}$ \\ 
$\frac{df}{dw_i}=$ \\
\subsection{$E(z)=\frac{1}{2}(t - f(z))^2$, $f(z)=\frac{1}{1+e^{-z}}$ and $z = \sum_{i=1}^n w_i x_i$.}
$\frac{dE}{dw_i}$
\section{Q4 Softmax Funciton}
\subsection{Where in NNs is the softmax function used and why?}
Softmax is used to normalize the outputs of NN to interval (0,1) and to make all components to add up to 1 so that they can be interpreted as regular probabilities. This tends to be implemented as the final step of a NN to get a probability distribution over all possible classes/predictions.  \\
\subsection{x is [1, 2, 3, -1, -4, 0], what is the value of softmax(x)}
[0.08607859048507978, 0.23398586833496002, 0.6360395340111326, 0.011649470423906664, 0.0005799929804444501, 0.03166654376447658]
\section{Q5:  FNN}
\subsection{How many connections (i.e., weights) are there in this network?}
\subsection{Given the input $x$, what is the formula for calculating the output of the first hidden layer?}

\subsection{Given the input $x$, what is the formula for calculating the output of the output layer?}

\section{Q6 MNIST NNs}
\subsection{What's the loss function used in the digit recognition task?}

\subsection{Why do they choose to minimize this function instead of maximizing classification accuracy?}

\subsection{In gradient descent, what's the formula for updating the weight matrix (or vector)? Why is that a good formula?}

\subsection{What are the main ideas and benefits of stochastic gradient descent?}

\subsection{What is a training epoch?}
A training epoch is one pass over the dataset batch size.
\subsection{Let $T$ be the size of the training data, $m$ be the size of mini-batch, and your training process contains $E$ training epoches. How many times is each weight in the NN updated? }
\subsection{How can one choose the learning rate?}

\subsection{What's the risk if the rate is too big?}
If the learning rate is too big then the network may never learn the function properly since a high learning rate can effectively skip over maximums
\subsection{What's the risk if the rate is too small?}
If the learning rate is too small then the network will train extremely slowly and it may never leave a local minum.
\section{Q7 MNSIT NN in practice}
\begin{table}[h]
\centering
\caption{Results on digit recognition}
\label{table1}
\begin{tabular}{|c|r|l|l|l|r|} \hline
  Expt id & \# of hidden neurons & epoch \# & mini batch size & learning rate & accuracy \\ \hline
  1  & 30   & 30 & 10 & 3.0 & 0.9461 \\ \hline
  2  & 10   & 30 & 10 & 3.0 & 0.9172 \\ \hline
  3  & 30   & 30 & 10 & 0.5 & 0.9403 \\ \hline
  4  & 30   & 30 & 10 & 10  & 0.9457 \\ \hline       
  5  & 30   & 30 & 100 & 3.0 & 0.9302 \\ \hline       
\end{tabular}
\end{table}  
 \end{document}
